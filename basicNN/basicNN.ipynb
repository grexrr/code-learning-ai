{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $\\vec{X} \\rightarrow \\vec{H} \\rightarrow \\vec{Y}$ with 3 -> 4 -> 2 node, \n",
    "\n",
    "to calculate the hidden layer tensor: \n",
    "\n",
    "$$h=\\sigma(W_{i}x + b)$$\n",
    "\n",
    "changes the input dimension from $3 \\times 1$ to $4 \\times 1$, therefore $\\vec{b}$ is also a $4 \\times 1$ matrix\n",
    "\n",
    "\n",
    "1. Hidden Layer matrix $\\vec{H}$ (4 $\\times$ 1)\n",
    "$$\n",
    "\\vec{H} = \\begin{bmatrix} h_1 \\\\ h_2 \\\\ h_3 \\\\ h_4 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "2. Input Layer $\\vec{X}$ (3 $\\times$ 1)\n",
    "$$\n",
    "\\vec{X} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "3. Weight matrix, $W_{1}$, therefore, is a ($4 \\times 3$)\n",
    "$$\n",
    "W_1 = \\begin{bmatrix} \n",
    "w_{11} & w_{12} & w_{13} \\\\\n",
    "w_{21} & w_{22} & w_{23} \\\\\n",
    "w_{31} & w_{32} & w_{33} \\\\\n",
    "w_{41} & w_{42} & w_{43}\n",
    "\\end{bmatrix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted result:\n",
      "[[ 7.93]\n",
      " [10.29]]\n"
     ]
    }
   ],
   "source": [
    "# true label y\n",
    "y_true = np.array([0.5, 0.8]).reshape(2, 1)\n",
    "\n",
    "# 3 nodes input layer\n",
    "X = np.array([1, 2, 3]).reshape(3, 1)\n",
    "\n",
    "# Hidden Layer parameter\n",
    "W_1 = np.array([\n",
    "    [0.1, 0.2, 0.3],   # 对应 h1 的权重\n",
    "    [0.4, 0.5, 0.6],   # 对应 h2 的权重\n",
    "    [0.7, 0.8, 0.9],   # 对应 h3 的权重\n",
    "    [1.0, 1.1, 1.2]    # 对应 h4 的权重\n",
    "])\n",
    "b_1 = np.array([0.1, 0.2, 0.3, 0.4]).reshape(4, 1)\n",
    "\n",
    "# Output Layer parameter\n",
    "W_2 = np.array([\n",
    "    [0.2, 0.4, 0.7, 0.3],   # 对应 o1 的权重\n",
    "    [0.3, 0.5, 0.2, 0.9],   # 对应 o2 的权重\n",
    "])\n",
    "b_2 = np.array([0.4, 0.6]).reshape(2, 1)\n",
    "\n",
    "# ----------------- Forward Propagation -----------------\n",
    "# Hidden Layer Calculation\n",
    "z_1 = W_1.dot(X) + b_1      # Linear Transformation\n",
    "H_1 = np.maximum(0, z_1)      # Relu\n",
    "\n",
    "# Output layer Calculation\n",
    "z_2 = W_2.dot(H_1) + b_2\n",
    "y_pred = np.maximum(0, z_2)\n",
    "\n",
    "print(\"predicted result:\")\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Calculation of $\\frac{\\partial_L}{\\partial_{W_2}}$ and $\\frac{\\partial_L}{\\partial_{d_2}}$ for Output Layer\n",
    "\n",
    "After forward propagation giving the output layer, backward propagation is required to calculate gradient and update the parameters.\n",
    "\n",
    "Gradient is a vector indicating the function's partial deriviative of each parameter at any point, it points toward where the function value ascending the fastest. \n",
    "\n",
    "In neural network, loss function $L$ has gradient $\\frac{\\partial_L}{\\partial_{y_{pred}}}$ and $\\frac{\\partial_L}{\\partial_{b}}$ telling us how to adjust weight $W$ and bias $b$ to optimize (minimize) the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward propagation loss:72.6325\n"
     ]
    }
   ],
   "source": [
    "# Calculate MSE(Mean Square Error) Loss\n",
    "loss = 0.5 * np.sum((y_pred - y_true) ** 2)\n",
    "print(\"forward propagation loss:\" + str(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case above, loss function MSE\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{2} sum(y_{pred} - y_{true})^2\n",
    "$$\n",
    "\n",
    "has\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{L}}{\\partial{y_{pred}}} = y_{pred} - y_{true}\n",
    "$$\n",
    "indicating gradient increases as $y_{pred}$ deviates from $y_{true}$, therefore it needs more adjustment.\n",
    "\n",
    "And we have: \n",
    "$$\n",
    "\\frac{\\partial y_{\\text{pred}}}{\\partial z_2} =\n",
    "\\begin{cases} \n",
    "1 & \\text{if } z_2 > 0, \\\\\n",
    "0 & \\text{else}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "We want: \n",
    "$$\\frac{\\partial{L}}{\\partial_{z_2}} = \\frac{\\partial{L}}{\\partial{y_{pred}}} \\times \\frac{\\partial y_{pred}}{\\partial {z_2}}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of Loss to y_predict\n",
      "[[7.43]\n",
      " [9.49]]\n",
      "ReLU deriviative matrix:\n",
      "[[1.]\n",
      " [1.]]\n",
      "Gradient of Loss to z_2:\n",
      "[[7.43]\n",
      " [9.49]]\n"
     ]
    }
   ],
   "source": [
    "dL_dypred = y_pred - y_true\n",
    "print(\"Gradient of Loss to y_predict\")\n",
    "print(dL_dypred)\n",
    "\n",
    "relu_deriv = (z_2 > 0).astype(float)\n",
    "print(\"ReLU deriviative matrix:\")\n",
    "print(relu_deriv)\n",
    "\n",
    "dL_dz2 = dL_dypred * relu_deriv \n",
    "print(\"Gradient of Loss to z_2:\")\n",
    "print(dL_dz2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then the gradient of $\\partial_{L}$ to $W_{2}$\n",
    "$$\n",
    "\\frac{\\partial_{L}}{\\partial_{W_{2}}} = \\frac{\\partial_{L}}{\\partial_{z_{2}}} \\times H_{1}^T\n",
    "$$\n",
    "\n",
    "where $H_{1}^T$ is the transpose of $H_{1}$, the hidden layer output. Since $\\frac{\\partial_{L}}{\\partial_{z_{2}}}$ is $(2 \\times 1)$ and $H_{1}$ is $1 \\times 4$, therefore $W_{2}$'s gradient $\\frac{\\partial_{L}}{\\partial_{W_{2}}}$ is $(2 \\times 4)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of Loss with respect to W2:\n",
      "[[11.145 25.262 39.379 53.496]\n",
      " [14.235 32.266 50.297 68.328]]\n"
     ]
    }
   ],
   "source": [
    "dL_dw2 = dL_dz2.dot(H_1.T)\n",
    "print(\"Gradient of Loss with respect to W2:\")\n",
    "print(dL_dw2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want \n",
    "$$\n",
    "\\frac{\\partial_{L}}{\\partial_{b_{2}}} = \\frac{\\partial_{L}}{\\partial_{z_{2}}} \\times \\frac{\\partial_{z_2}}{\\partial_{b_{2}}}\n",
    "$$\n",
    "\n",
    "while \n",
    "\n",
    "$$\n",
    "z_{2} = W_2 * H_1 + b_2\n",
    "$$\n",
    "\n",
    "therefore\n",
    "\n",
    "$$\n",
    "\\frac{\\partial_{z_2}}{\\partial_{b_{2}}} = 1\n",
    "$$\n",
    "\n",
    "resulting \n",
    "\n",
    "$$\n",
    "\\frac{\\partial_{L}}{\\partial_{b_{2}}} = \\frac{\\partial_{L}}{\\partial_{z_{2}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of Loss with respect to d2:\n",
      "[[7.43]\n",
      " [9.49]]\n"
     ]
    }
   ],
   "source": [
    "dl_db2 = dL_dz2\n",
    "print(\"Gradient of Loss with respect to d2:\")\n",
    "print(dl_db2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Calculation of $\\frac{\\partial_L}{\\partial_{W_{1}}}$ and $\\frac{\\partial_L}{\\partial_{b_{1}}}$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial_L}{\\partial_{W_{1}}} = \\frac{\\partial_L}{\\partial_{H_{1}}} \\times \\frac{\\partial_{H_{1}}}{\\partial_{z_1}} \\times \\frac{\\partial_{z_1}}{\\partial_{W_1}}\n",
    "$$\n",
    "\n",
    "Where\n",
    "\n",
    "$$\n",
    "\\frac{\\partial_L}{\\partial_{H_{1}}} = W_{2}^T \\times \\frac{\\partial_L}{\\partial_{z_{2}}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial_{H_1}}{\\partial_{z_{1}}} = \n",
    "\\begin{cases} \n",
    "1 & \\text{if } z_2 > 0, \\\\\n",
    "0 & \\text{else}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial_{z_1}}{\\partial_{W_{1}}} = X^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dL_dH1 = W_2.T.dot(dL_dz2)\n",
    "relu_deriv_z1 = (z_1 > 0).astype(float)\n",
    "dL_dz1 = dL_dH1 * relu_deriv_z1 \n",
    "dL_dW1 = dL_dz1.dot(X.T) \n",
    "dL_db1 = dL_dz1  # (4x1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
