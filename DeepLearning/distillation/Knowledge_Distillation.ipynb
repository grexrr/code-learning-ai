{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Comparing Traditional and Enhanced Step-by-Step Distillation: Adaptive Learning, Cosine Similarity, and Curriculum-Based Rationale Supervision**\n",
        "\n",
        "\n",
        "Shenggang **Li**"
      ],
      "metadata": {
        "id": "swk0KTYzqdIX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "71gGbvxYqal3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-Hn3FfHnzkN",
        "outputId": "c7335314-5d12-464b-d199-6929d762ef7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-18 14:26:00--  https://raw.githubusercontent.com/datalev001/distillation/main/data/distdata.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11407870 (11M) [text/plain]\n",
            "Saving to: ‘distdata.csv’\n",
            "\n",
            "distdata.csv        100%[===================>]  10.88M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2025-03-18 14:26:01 (122 MB/s) - ‘distdata.csv’ saved [11407870/11407870]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Set random seed to ensure reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Hyperparameter settings\n",
        "num_samples = 100000    # Number of data rows\n",
        "num_features = 10       # Number of features\n",
        "batch_size = 512        # Mini-batch size\n",
        "epochs_teacher = 5      # Number of training epochs for the teacher model\n",
        "epochs_student = 10     # Number of training epochs for the student model\n",
        "learning_rate = 0.01    # Learning rate\n",
        "T = 2.0                 # Temperature parameter (used for probability smoothing)\n",
        "alpha = 0.5             # Weight of hard labels and soft labels in traditional distillation (range 0~1)\n",
        "# For step-by-step distillation, beta is the weight of \"reasoning process loss\"\n",
        "beta = 0.3\n",
        "\n",
        "!wget -O distdata.csv https://raw.githubusercontent.com/datalev001/distillation/main/data/distdata.csv\n",
        "\n",
        "#csv_file = \"distdata.csv\"\n",
        "\n",
        "# Read data from CSV\n",
        "df = pd.read_csv(csv_file, sep=',')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "# Extract features and labels\n",
        "X_df = df.drop(columns=[\"label\"]).values.astype(np.float32)\n",
        "y_df = df[\"label\"].values.astype(np.float32)\n",
        "\n",
        "# Convert to torch tensors\n",
        "X = torch.tensor(X_df)\n",
        "y = torch.tensor(y_df)\n",
        "\n",
        "# Construct DataLoader\n",
        "dataset = TensorDataset(X, y)\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# 2. Define a Simple Logistic Regression Model\n",
        "# ---------------------------\n",
        "class LogisticRegressionModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(LogisticRegressionModel, self).__init__()\n",
        "        # Using a fully connected layer to simulate logistic regression\n",
        "        self.linear = nn.Linear(input_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Return raw scores (logits), apply sigmoid later to get probabilities\n",
        "        return self.linear(x)\n",
        "\n",
        "# ---------------------------\n",
        "# 3. Train Teacher Model\n",
        "# ---------------------------\n",
        "def train_teacher(model, dataloader, epochs, lr):\n",
        "    model.train()\n",
        "    criterion = nn.BCEWithLogitsLoss()  # Built-in binary cross-entropy loss (with internal sigmoid)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(inputs).squeeze(1)  # shape: (batch_size)\n",
        "            loss = criterion(logits, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        avg_loss = running_loss / len(dataloader)\n",
        "        print(f\"Teacher Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# ---------------------------\n",
        "# 4. Generate Teacher Model's Soft Labels (Traditional Distillation)\n",
        "# ---------------------------\n",
        "def generate_teacher_outputs(model, dataloader, temperature):\n",
        "    model.eval()\n",
        "    teacher_logits_all = []\n",
        "    teacher_probs_all = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, _ in dataloader:\n",
        "            logits = model(inputs).squeeze(1)  # Raw logits\n",
        "            teacher_logits_all.append(logits)\n",
        "            # Soft labels: Apply temperature T for smoothing (divide by T first, then apply sigmoid)\n",
        "            soft_probs = torch.sigmoid(logits / temperature)\n",
        "            teacher_probs_all.append(soft_probs)\n",
        "    teacher_logits = torch.cat(teacher_logits_all)\n",
        "    teacher_probs = torch.cat(teacher_probs_all)\n",
        "    return teacher_logits, teacher_probs\n",
        "\n",
        "# ---------------------------\n",
        "# 5. Traditional Distillation: Train Student Model\n",
        "# ---------------------------\n",
        "def train_student_traditional(teacher_probs_all, model_teacher, model_student, dataloader, epochs, lr, temperature, alpha):\n",
        "    model_student.train()\n",
        "    optimizer = optim.SGD(model_student.parameters(), lr=lr)\n",
        "    # Define hard label loss (standard binary cross-entropy)\n",
        "    hard_criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    # Define KL divergence function for binary classification\n",
        "    def kd_loss(student_logits, teacher_probs):\n",
        "        # Compute student probabilities with temperature scaling\n",
        "        student_probs = torch.sigmoid(student_logits / temperature)\n",
        "        # Compute KL divergence: p_teacher * log(p_teacher / p_student) + (1 - p_teacher) * log((1 - p_teacher) / (1 - p_student))\n",
        "        # To prevent log(0), add a small value eps\n",
        "        eps = 1e-7\n",
        "        student_probs = torch.clamp(student_probs, eps, 1 - eps)\n",
        "        teacher_probs = torch.clamp(teacher_probs, eps, 1 - eps)\n",
        "        kl = teacher_probs * torch.log(teacher_probs / student_probs) + (1 - teacher_probs) * torch.log((1 - teacher_probs) / (1 - student_probs))\n",
        "        return torch.mean(kl)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
        "            optimizer.zero_grad()\n",
        "            student_logits = model_student(inputs).squeeze(1)\n",
        "            # Compute hard label loss: student model directly predicts (without temperature scaling)\n",
        "            loss_hard = hard_criterion(student_logits, targets)\n",
        "            # Compute distillation loss: use soft labels from teacher with temperature T\n",
        "            # Assume the data order is consistent for simplicity\n",
        "            # Note: In practice, additional alignment may be required; here we directly use precomputed teacher outputs\n",
        "            # Extract corresponding soft teacher probabilities using the order of dataloader\n",
        "            # Simulate extracting soft teacher labels using batch index and batch size\n",
        "            start_idx = batch_idx * batch_size\n",
        "            end_idx = start_idx + inputs.size(0)\n",
        "            teacher_soft = teacher_probs_all[start_idx:end_idx].to(inputs.device)\n",
        "\n",
        "            loss_kd = kd_loss(student_logits, teacher_soft)\n",
        "\n",
        "            # Total loss: Weighted sum of hard label loss and distillation loss\n",
        "            loss = alpha * loss_hard + (1 - alpha) * (temperature**2) * loss_kd\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Print intermediate results for each batch (for educational purposes)\n",
        "            if batch_idx % 200 == 0:\n",
        "                print(f\"Traditional Distillation: Epoch {epoch+1} Batch {batch_idx}, \"\n",
        "                      f\"Loss_hard: {loss_hard.item():.4f}, Loss_KD: {loss_kd.item():.4f}\")\n",
        "        avg_loss = running_loss / len(dataloader)\n",
        "        print(f\"Traditional Distillation: Epoch [{epoch+1}/{epochs}], Total Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    return model_student\n",
        "\n",
        "# ---------------------------\n",
        "# 6. Step-by-Step Distillation: Train Student Model (Learn both Answers and Reasoning Process)\n",
        "# ---------------------------\n",
        "def train_student_step_by_step(teacher_logits_all, teacher_probs_all, model_teacher, model_student, dataloader, epochs, lr, temperature, alpha, beta):\n",
        "    model_student.train()\n",
        "    optimizer = optim.SGD(model_student.parameters(), lr=lr)\n",
        "    # Define hard label loss (Standard Binary Cross-Entropy)\n",
        "    hard_criterion = nn.BCEWithLogitsLoss()\n",
        "    # Define reasoning process loss (Mean Squared Error, mimicking teacher’s linear scores)\n",
        "    mse_loss = nn.MSELoss()\n",
        "\n",
        "    def kd_loss(student_logits, teacher_probs):\n",
        "        student_probs = torch.sigmoid(student_logits / temperature)\n",
        "        eps = 1e-7\n",
        "        student_probs = torch.clamp(student_probs, eps, 1 - eps)\n",
        "        teacher_probs = torch.clamp(teacher_probs, eps, 1 - eps)\n",
        "        kl = teacher_probs * torch.log(teacher_probs / student_probs) + (1 - teacher_probs) * torch.log((1 - teacher_probs) / (1 - student_probs))\n",
        "        return torch.mean(kl)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
        "            optimizer.zero_grad()\n",
        "            student_logits = model_student(inputs).squeeze(1)\n",
        "            # Compute hard label loss\n",
        "            loss_hard = hard_criterion(student_logits, targets)\n",
        "            # Compute distillation loss (soft label matching)\n",
        "            start_idx = batch_idx * batch_size\n",
        "            end_idx = start_idx + inputs.size(0)\n",
        "            teacher_soft = teacher_probs_all[start_idx:end_idx].to(inputs.device)\n",
        "            loss_kd = kd_loss(student_logits, teacher_soft)\n",
        "            # Compute reasoning process loss (MSE, match teacher and student linear scores)\n",
        "            teacher_logits_batch = teacher_logits_all[start_idx:end_idx].to(inputs.device)\n",
        "            loss_rationale = mse_loss(student_logits, teacher_logits_batch)\n",
        "\n",
        "            # Total loss: Combine hard label loss, distillation loss, and reasoning process loss\n",
        "            loss = alpha * loss_hard + beta * loss_rationale + (1 - alpha - beta) * (temperature**2) * loss_kd\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Print intermediate results for each batch (for educational purposes)\n",
        "            if batch_idx % 200 == 0:\n",
        "                print(f\"Step-by-Step Distillation: Epoch {epoch+1} Batch {batch_idx}, \"\n",
        "                      f\"Loss_hard: {loss_hard.item():.4f}, Loss_rationale: {loss_rationale.item():.4f}, Loss_KD: {loss_kd.item():.4f}\")\n",
        "        avg_loss = running_loss / len(dataloader)\n",
        "        print(f\"Step-by-Step Distillation: Epoch [{epoch+1}/{epochs}], Total Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    return model_student\n",
        "\n",
        "# ---------------------------\n",
        "# 7. Main Function: Execute Teacher Training, Generate Teacher Outputs, and Train Two Student Models Separately\n",
        "# ---------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Train Teacher Model\n",
        "    print(\"=== Training Teacher Model ===\")\n",
        "    teacher_model = LogisticRegressionModel(num_features)\n",
        "    teacher_model = train_teacher(teacher_model, data_loader, epochs_teacher, learning_rate)\n",
        "\n",
        "    # Generate Teacher Model's Intermediate Outputs (logits) and Soft Labels (smoothed with temperature T)\n",
        "    teacher_logits_all, teacher_probs_all = generate_teacher_outputs(teacher_model, data_loader, T)\n",
        "    print(\"Teacher model generated soft label samples:\", teacher_probs_all[:5])\n",
        "\n",
        "    # Copy soft labels into numpy arrays for indexing (keep data order consistent with DataLoader)\n",
        "    teacher_probs_all = teacher_probs_all.cpu()\n",
        "    teacher_logits_all = teacher_logits_all.cpu()\n",
        "\n",
        "    # Train Student Model via Traditional Distillation\n",
        "    print(\"\\n=== Training Student Model via Traditional Distillation ===\")\n",
        "    student_model_traditional = LogisticRegressionModel(num_features)\n",
        "    student_model_traditional = train_student_traditional(teacher_probs_all, teacher_model, student_model_traditional,\n",
        "                                                          data_loader, epochs_student, learning_rate, T, alpha)\n",
        "\n",
        "    # Train Student Model via Step-by-Step Distillation (Learn both Predictions and Reasoning Process)\n",
        "    print(\"\\n=== Training Student Model via Step-by-Step Distillation ===\")\n",
        "    student_model_step_by_step = LogisticRegressionModel(num_features)\n",
        "    student_model_step_by_step = train_student_step_by_step(teacher_logits_all, teacher_probs_all, teacher_model,\n",
        "                                                            student_model_step_by_step, data_loader, epochs_student,\n",
        "                                                            learning_rate, T, alpha, beta)\n",
        "\n",
        "    # Test Model Predictions on Training Data (Educational Example)\n",
        "    student_model_traditional.eval()\n",
        "    student_model_step_by_step.eval()\n",
        "    with torch.no_grad():\n",
        "        sample_inputs, sample_targets = next(iter(data_loader))\n",
        "        # Traditional Distillation Student Model Predictions\n",
        "        logits_trad = student_model_traditional(sample_inputs).squeeze(1)\n",
        "        preds_trad = torch.sigmoid(logits_trad)\n",
        "        # Step-by-Step Distillation Student Model Predictions\n",
        "        logits_step = student_model_step_by_step(sample_inputs).squeeze(1)\n",
        "        preds_step = torch.sigmoid(logits_step)\n",
        "        print(\"\\nTraditional Distillation Student Model Predictions (First 10 Samples):\")\n",
        "        for i in range(10):\n",
        "            print(f\"Sample {i+1}: True Label: {sample_targets[i].item():.0f}, Student Prediction Probability: {preds_trad[i].item():.4f}\")\n",
        "        print(\"\\nStep-by-Step Distillation Student Model Predictions (First 10 Samples):\")\n",
        "        for i in range(10):\n",
        "            print(f\"Sample {i+1}: True Label: {sample_targets[i].item():.0f}, Student Prediction Probability: {preds_step[i].item():.4f}\")\n",
        "\n",
        "\n",
        "########update###########\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Hyperparameters\n",
        "num_samples = 100000    # Number of samples\n",
        "num_features = 10       # Number of features\n",
        "batch_size = 512        # Mini-batch size\n",
        "epochs_teacher = 5      # Teacher model training epochs\n",
        "epochs_student = 10     # Student model training epochs\n",
        "learning_rate = 0.01    # Learning rate for teacher and traditional student\n",
        "student_lr = 0.005      # Learning rate for step-by-step student\n",
        "T = 2.0                 # Temperature parameter (for smoothing probabilities)\n",
        "alpha = 0.5             # Weight for hard label loss in distillation\n",
        "beta_initial = 0.1      # Maximum beta value for rationale loss (reduced from 0.2)\n",
        "consistency_lambda = 0.25  # Increased weight for consistency regularization\n",
        "noise_std = 0.04         # Increased noise magnitude for consistency regularization\n",
        "curriculum_epochs = 8    # Ramp-up period for rationale loss (extended to 8 epochs)\n",
        "rationale_margin = 0.1   # Margin for cosine similarity loss\n",
        "\n",
        "# Read data from CSV\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "# Extract features and labels\n",
        "X_df = df.drop(columns=[\"label\"]).values.astype(np.float32)\n",
        "y_df = df[\"label\"].values.astype(np.float32)\n",
        "\n",
        "# Convert to torch tensors\n",
        "X = torch.tensor(X_df)\n",
        "y = torch.tensor(y_df)\n",
        "\n",
        "# Construct DataLoader\n",
        "dataset = TensorDataset(X, y)\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# ---------------------------\n",
        "# 2. Define a Simple Logistic Regression Model\n",
        "# ---------------------------\n",
        "class LogisticRegressionModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(LogisticRegressionModel, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Return raw logits; apply sigmoid later for probabilities\n",
        "        return self.linear(x)\n",
        "\n",
        "# ---------------------------\n",
        "# 3. Train Teacher Model\n",
        "# ---------------------------\n",
        "def train_teacher(model, dataloader, epochs, lr):\n",
        "    model.train()\n",
        "    criterion = nn.BCEWithLogitsLoss()  # Includes sigmoid internally\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, targets in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(inputs).squeeze(1)\n",
        "            loss = criterion(logits, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        avg_loss = running_loss / len(dataloader)\n",
        "        print(f\"Teacher Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# ---------------------------\n",
        "# 4. Generate Teacher Outputs (Soft Labels)\n",
        "# ---------------------------\n",
        "def generate_teacher_outputs(model, dataloader, temperature):\n",
        "    model.eval()\n",
        "    teacher_logits_all = []\n",
        "    teacher_probs_all = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, _ in dataloader:\n",
        "            logits = model(inputs).squeeze(1)\n",
        "            teacher_logits_all.append(logits)\n",
        "            soft_probs = torch.sigmoid(logits / temperature)\n",
        "            teacher_probs_all.append(soft_probs)\n",
        "    teacher_logits = torch.cat(teacher_logits_all)\n",
        "    teacher_probs = torch.cat(teacher_probs_all)\n",
        "    return teacher_logits, teacher_probs\n",
        "\n",
        "# ---------------------------\n",
        "# 5. Traditional Distillation: Train Student Model\n",
        "# ---------------------------\n",
        "def train_student_traditional(teacher_probs_all, model_teacher, model_student, dataloader, epochs, lr, temperature, alpha):\n",
        "    model_student.train()\n",
        "    optimizer = optim.SGD(model_student.parameters(), lr=lr)\n",
        "    hard_criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def kd_loss(student_logits, teacher_probs):\n",
        "        student_probs = torch.sigmoid(student_logits / temperature)\n",
        "        eps = 1e-7\n",
        "        student_probs = torch.clamp(student_probs, eps, 1 - eps)\n",
        "        teacher_probs = torch.clamp(teacher_probs, eps, 1 - eps)\n",
        "        kl = teacher_probs * torch.log(teacher_probs / student_probs) + \\\n",
        "             (1 - teacher_probs) * torch.log((1 - teacher_probs) / (1 - student_probs))\n",
        "        return torch.mean(kl)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
        "            optimizer.zero_grad()\n",
        "            student_logits = model_student(inputs).squeeze(1)\n",
        "            loss_hard = hard_criterion(student_logits, targets)\n",
        "            start_idx = batch_idx * batch_size\n",
        "            end_idx = start_idx + inputs.size(0)\n",
        "            teacher_soft = teacher_probs_all[start_idx:end_idx].to(inputs.device)\n",
        "            loss_kd = kd_loss(student_logits, teacher_soft)\n",
        "            loss = alpha * loss_hard + (1 - alpha) * (temperature**2) * loss_kd\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            if batch_idx % 200 == 0:\n",
        "                print(f\"Traditional Distillation: Epoch {epoch+1} Batch {batch_idx}, \"\n",
        "                      f\"Loss_hard: {loss_hard.item():.4f}, Loss_KD: {loss_kd.item():.4f}\")\n",
        "        avg_loss = running_loss / len(dataloader)\n",
        "        print(f\"Traditional Distillation: Epoch [{epoch+1}/{epochs}], Total Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    return model_student\n",
        "\n",
        "# ---------------------------\n",
        "# 6. Step-by-Step Distillation: Train Student Model\n",
        "#     (with Adaptive Rationale Weighting using Linear Ramp-Up, Margin-based Cosine Rationale Loss,\n",
        "#      Consistency Regularization, and Learning Rate Scheduling)\n",
        "# ---------------------------\n",
        "def train_student_step_by_step(teacher_logits_all, teacher_probs_all, model_teacher, model_student, dataloader, epochs, lr, temperature, alpha, beta_initial, consistency_lambda):\n",
        "    model_student.train()\n",
        "    optimizer = optim.Adam(model_student.parameters(), lr=lr)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
        "    hard_criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def kd_loss(student_logits, teacher_probs):\n",
        "        student_probs = torch.sigmoid(student_logits / temperature)\n",
        "        eps = 1e-7\n",
        "        student_probs = torch.clamp(student_probs, eps, 1 - eps)\n",
        "        teacher_probs = torch.clamp(teacher_probs, eps, 1 - eps)\n",
        "        kl = teacher_probs * torch.log(teacher_probs / student_probs) + \\\n",
        "             (1 - teacher_probs) * torch.log((1 - teacher_probs) / (1 - student_probs))\n",
        "        return torch.mean(kl)\n",
        "\n",
        "    # Margin-based cosine similarity loss for rationale:\n",
        "    cosine_loss = nn.CosineEmbeddingLoss(margin=rationale_margin)\n",
        "\n",
        "    # Linear ramp-up for current_beta over curriculum_epochs\n",
        "    def rampup(epoch, max_epochs, max_beta):\n",
        "        return max_beta * min(1.0, epoch / max_epochs)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "\n",
        "        current_beta = rampup(epoch, curriculum_epochs, beta_initial)\n",
        "\n",
        "        for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
        "            optimizer.zero_grad()\n",
        "            student_logits = model_student(inputs).squeeze(1)\n",
        "            loss_hard = hard_criterion(student_logits, targets)\n",
        "\n",
        "            start_idx = batch_idx * batch_size\n",
        "            end_idx = start_idx + inputs.size(0)\n",
        "            teacher_soft = teacher_probs_all[start_idx:end_idx].to(inputs.device)\n",
        "            loss_kd = kd_loss(student_logits, teacher_soft)\n",
        "\n",
        "            teacher_logits_batch = teacher_logits_all[start_idx:end_idx].to(inputs.device)\n",
        "            # For cosine embedding loss, we need to reshape to (batch_size, 1) and target of 1\n",
        "            cos_target = torch.ones(student_logits.size()).to(inputs.device)\n",
        "            loss_rationale = cosine_loss(student_logits.unsqueeze(1), teacher_logits_batch.unsqueeze(1), cos_target)\n",
        "\n",
        "            # Consistency Regularization: perturb inputs and enforce similar predictions\n",
        "            noise = torch.randn_like(inputs) * noise_std\n",
        "            student_logits_perturbed = model_student(inputs + noise).squeeze(1)\n",
        "            # Here, we use KL divergence between the predictions (after sigmoid)\n",
        "            preds = torch.sigmoid(student_logits)\n",
        "            preds_perturbed = torch.sigmoid(student_logits_perturbed)\n",
        "            eps = 1e-7\n",
        "            preds = torch.clamp(preds, eps, 1 - eps)\n",
        "            preds_perturbed = torch.clamp(preds_perturbed, eps, 1 - eps)\n",
        "            loss_consistency = torch.mean(preds * torch.log(preds / preds_perturbed) +\n",
        "                                            (1 - preds) * torch.log((1 - preds) / (1 - preds_perturbed)))\n",
        "\n",
        "            loss_total = (alpha * loss_hard +\n",
        "                          current_beta * loss_rationale +\n",
        "                          (1 - alpha - current_beta) * (temperature**2) * loss_kd +\n",
        "                          consistency_lambda * loss_consistency)\n",
        "\n",
        "            loss_total.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss_total.item()\n",
        "\n",
        "            if batch_idx % 200 == 0:\n",
        "                print(f\"Step-by-Step Distillation: Epoch {epoch+1} Batch {batch_idx}, \"\n",
        "                      f\"Loss_hard: {loss_hard.item():.4f}, Loss_rationale: {loss_rationale.item():.4f}, \"\n",
        "                      f\"Loss_KD: {loss_kd.item():.4f}, Loss_consistency: {loss_consistency.item():.4f}, \"\n",
        "                      f\"Current_beta: {current_beta:.4f}\")\n",
        "        avg_loss = running_loss / len(dataloader)\n",
        "        print(f\"Step-by-Step Distillation: Epoch [{epoch+1}/{epochs}], Total Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Step learning rate scheduler\n",
        "        scheduler.step(avg_loss)\n",
        "\n",
        "    return model_student\n",
        "\n",
        "# ---------------------------\n",
        "# 7. Main: Train Teacher, Generate Outputs, and Train Student Models\n",
        "# ---------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Train teacher model\n",
        "    print(\"=== Training Teacher Model ===\")\n",
        "    teacher_model = LogisticRegressionModel(num_features)\n",
        "    teacher_model = train_teacher(teacher_model, data_loader, epochs_teacher, learning_rate)\n",
        "\n",
        "    # Generate teacher outputs (logits and soft labels) using temperature scaling\n",
        "    teacher_logits_all, teacher_probs_all = generate_teacher_outputs(teacher_model, data_loader, T)\n",
        "    print(\"Teacher model soft label samples:\", teacher_probs_all[:5])\n",
        "\n",
        "    # Ensure teacher outputs are on CPU for consistent indexing\n",
        "    teacher_probs_all = teacher_probs_all.cpu()\n",
        "    teacher_logits_all = teacher_logits_all.cpu()\n",
        "\n",
        "    # Traditional distillation training for student model\n",
        "    print(\"\\n=== Training Student Model via Traditional Distillation ===\")\n",
        "    student_model_traditional = LogisticRegressionModel(num_features)\n",
        "    student_model_traditional = train_student_traditional(teacher_probs_all, teacher_model, student_model_traditional,\n",
        "                                                          data_loader, epochs_student, learning_rate, T, alpha)\n",
        "\n",
        "    # Step-by-Step distillation training for student model\n",
        "    # (with adaptive rationale loss weight (linear ramp-up over 8 epochs), margin-based cosine rationale loss,\n",
        "    #  consistency regularization, and learning rate scheduling)\n",
        "    print(\"\\n=== Training Student Model via Step-by-Step Distillation ===\")\n",
        "    student_model_step_by_step = LogisticRegressionModel(num_features)\n",
        "    student_model_step_by_step = train_student_step_by_step(teacher_logits_all, teacher_probs_all, teacher_model,\n",
        "                                                            student_model_step_by_step, data_loader, epochs_student,\n",
        "                                                            student_lr, T, alpha, beta_initial, consistency_lambda)\n",
        "\n",
        "    # Testing: Display predictions for some samples from each student model\n",
        "    student_model_traditional.eval()\n",
        "    student_model_step_by_step.eval()\n",
        "    with torch.no_grad():\n",
        "        sample_inputs, sample_targets = next(iter(data_loader))\n",
        "        logits_trad = student_model_traditional(sample_inputs).squeeze(1)\n",
        "        preds_trad = torch.sigmoid(logits_trad)\n",
        "        logits_step = student_model_step_by_step(sample_inputs).squeeze(1)\n",
        "        preds_step = torch.sigmoid(logits_step)\n",
        "        print(\"\\nTraditional Distillation Student Predictions (first 10 samples):\")\n",
        "        for i in range(10):\n",
        "            print(f\"Sample {i+1}: True Label: {sample_targets[i].item():.0f}, Prediction: {preds_trad[i].item():.4f}\")\n",
        "        print(\"\\nStep-by-Step Distillation Student Predictions (first 10 samples):\")\n",
        "        for i in range(10):\n",
        "            print(f\"Sample {i+1}: True Label: {sample_targets[i].item():.0f}, Prediction: {preds_step[i].item():.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbjVFPV6n0TN",
        "outputId": "2951ccba-1d4e-42e4-e467-53cc428c792b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Training Teacher Model ===\n",
            "Teacher Epoch [1/5], Loss: 0.5858\n",
            "Teacher Epoch [2/5], Loss: 0.4388\n",
            "Teacher Epoch [3/5], Loss: 0.3657\n",
            "Teacher Epoch [4/5], Loss: 0.3222\n",
            "Teacher Epoch [5/5], Loss: 0.2927\n",
            "Teacher model generated soft label samples: tensor([0.8456, 0.3090, 0.7469, 0.7839, 0.6946])\n",
            "\n",
            "=== Training Student Model via Traditional Distillation ===\n",
            "Traditional Distillation: Epoch 1 Batch 0, Loss_hard: 0.7228, Loss_KD: 0.1044\n",
            "Traditional Distillation: Epoch [1/10], Total Loss: 0.5078\n",
            "Traditional Distillation: Epoch 2 Batch 0, Loss_hard: 0.5849, Loss_KD: 0.0945\n",
            "Traditional Distillation: Epoch [2/10], Total Loss: 0.4553\n",
            "Traditional Distillation: Epoch 3 Batch 0, Loss_hard: 0.5144, Loss_KD: 0.0940\n",
            "Traditional Distillation: Epoch [3/10], Total Loss: 0.4350\n",
            "Traditional Distillation: Epoch 4 Batch 0, Loss_hard: 0.4675, Loss_KD: 0.0959\n",
            "Traditional Distillation: Epoch [4/10], Total Loss: 0.4254\n",
            "Traditional Distillation: Epoch 5 Batch 0, Loss_hard: 0.4678, Loss_KD: 0.0993\n",
            "Traditional Distillation: Epoch [5/10], Total Loss: 0.4221\n",
            "Traditional Distillation: Epoch 6 Batch 0, Loss_hard: 0.4367, Loss_KD: 0.0983\n",
            "Traditional Distillation: Epoch [6/10], Total Loss: 0.4207\n",
            "Traditional Distillation: Epoch 7 Batch 0, Loss_hard: 0.4461, Loss_KD: 0.1000\n",
            "Traditional Distillation: Epoch [7/10], Total Loss: 0.4198\n",
            "Traditional Distillation: Epoch 8 Batch 0, Loss_hard: 0.4279, Loss_KD: 0.1071\n",
            "Traditional Distillation: Epoch [8/10], Total Loss: 0.4200\n",
            "Traditional Distillation: Epoch 9 Batch 0, Loss_hard: 0.4431, Loss_KD: 0.0966\n",
            "Traditional Distillation: Epoch [9/10], Total Loss: 0.4205\n",
            "Traditional Distillation: Epoch 10 Batch 0, Loss_hard: 0.4226, Loss_KD: 0.1009\n",
            "Traditional Distillation: Epoch [10/10], Total Loss: 0.4204\n",
            "\n",
            "=== Training Student Model via Step-by-Step Distillation ===\n",
            "Step-by-Step Distillation: Epoch 1 Batch 0, Loss_hard: 0.6940, Loss_rationale: 3.6565, Loss_KD: 0.0881\n",
            "Step-by-Step Distillation: Epoch [1/10], Total Loss: 1.4378\n",
            "Step-by-Step Distillation: Epoch 2 Batch 0, Loss_hard: 0.6108, Loss_rationale: 3.5336, Loss_KD: 0.0841\n",
            "Step-by-Step Distillation: Epoch [2/10], Total Loss: 1.3934\n",
            "Step-by-Step Distillation: Epoch 3 Batch 0, Loss_hard: 0.5847, Loss_rationale: 3.4874, Loss_KD: 0.0828\n",
            "Step-by-Step Distillation: Epoch [3/10], Total Loss: 1.3915\n",
            "Step-by-Step Distillation: Epoch 4 Batch 0, Loss_hard: 0.5823, Loss_rationale: 3.4554, Loss_KD: 0.0821\n",
            "Step-by-Step Distillation: Epoch [4/10], Total Loss: 1.3916\n",
            "Step-by-Step Distillation: Epoch 5 Batch 0, Loss_hard: 0.5864, Loss_rationale: 3.4947, Loss_KD: 0.0832\n",
            "Step-by-Step Distillation: Epoch [5/10], Total Loss: 1.3911\n",
            "Step-by-Step Distillation: Epoch 6 Batch 0, Loss_hard: 0.5822, Loss_rationale: 3.4782, Loss_KD: 0.0826\n",
            "Step-by-Step Distillation: Epoch [6/10], Total Loss: 1.3911\n",
            "Step-by-Step Distillation: Epoch 7 Batch 0, Loss_hard: 0.5810, Loss_rationale: 3.4826, Loss_KD: 0.0829\n",
            "Step-by-Step Distillation: Epoch [7/10], Total Loss: 1.3907\n",
            "Step-by-Step Distillation: Epoch 8 Batch 0, Loss_hard: 0.5777, Loss_rationale: 3.4194, Loss_KD: 0.0814\n",
            "Step-by-Step Distillation: Epoch [8/10], Total Loss: 1.3908\n",
            "Step-by-Step Distillation: Epoch 9 Batch 0, Loss_hard: 0.5830, Loss_rationale: 3.4115, Loss_KD: 0.0810\n",
            "Step-by-Step Distillation: Epoch [9/10], Total Loss: 1.3900\n",
            "Step-by-Step Distillation: Epoch 10 Batch 0, Loss_hard: 0.5858, Loss_rationale: 3.4295, Loss_KD: 0.0812\n",
            "Step-by-Step Distillation: Epoch [10/10], Total Loss: 1.3905\n",
            "\n",
            "Traditional Distillation Student Model Predictions (First 10 Samples):\n",
            "Sample 1: True Label: 0, Student Prediction Probability: 0.3837\n",
            "Sample 2: True Label: 1, Student Prediction Probability: 0.7153\n",
            "Sample 3: True Label: 0, Student Prediction Probability: 0.3561\n",
            "Sample 4: True Label: 1, Student Prediction Probability: 0.7547\n",
            "Sample 5: True Label: 0, Student Prediction Probability: 0.2788\n",
            "Sample 6: True Label: 1, Student Prediction Probability: 0.8358\n",
            "Sample 7: True Label: 1, Student Prediction Probability: 0.7652\n",
            "Sample 8: True Label: 1, Student Prediction Probability: 0.7584\n",
            "Sample 9: True Label: 0, Student Prediction Probability: 0.4764\n",
            "Sample 10: True Label: 1, Student Prediction Probability: 0.7973\n",
            "\n",
            "Step-by-Step Distillation Student Model Predictions (First 10 Samples):\n",
            "Sample 1: True Label: 0, Student Prediction Probability: 0.5489\n",
            "Sample 2: True Label: 1, Student Prediction Probability: 0.6502\n",
            "Sample 3: True Label: 0, Student Prediction Probability: 0.5404\n",
            "Sample 4: True Label: 1, Student Prediction Probability: 0.6613\n",
            "Sample 5: True Label: 0, Student Prediction Probability: 0.5109\n",
            "Sample 6: True Label: 1, Student Prediction Probability: 0.6945\n",
            "Sample 7: True Label: 1, Student Prediction Probability: 0.6686\n",
            "Sample 8: True Label: 1, Student Prediction Probability: 0.6647\n",
            "Sample 9: True Label: 0, Student Prediction Probability: 0.5732\n",
            "Sample 10: True Label: 1, Student Prediction Probability: 0.6793\n",
            "=== Training Teacher Model ===\n",
            "Teacher Epoch [1/5], Loss: 0.5858\n",
            "Teacher Epoch [2/5], Loss: 0.4388\n",
            "Teacher Epoch [3/5], Loss: 0.3657\n",
            "Teacher Epoch [4/5], Loss: 0.3222\n",
            "Teacher Epoch [5/5], Loss: 0.2927\n",
            "Teacher model soft label samples: tensor([0.8456, 0.3090, 0.7469, 0.7839, 0.6946])\n",
            "\n",
            "=== Training Student Model via Traditional Distillation ===\n",
            "Traditional Distillation: Epoch 1 Batch 0, Loss_hard: 0.7228, Loss_KD: 0.1044\n",
            "Traditional Distillation: Epoch [1/10], Total Loss: 0.5078\n",
            "Traditional Distillation: Epoch 2 Batch 0, Loss_hard: 0.5849, Loss_KD: 0.0945\n",
            "Traditional Distillation: Epoch [2/10], Total Loss: 0.4553\n",
            "Traditional Distillation: Epoch 3 Batch 0, Loss_hard: 0.5144, Loss_KD: 0.0940\n",
            "Traditional Distillation: Epoch [3/10], Total Loss: 0.4350\n",
            "Traditional Distillation: Epoch 4 Batch 0, Loss_hard: 0.4675, Loss_KD: 0.0959\n",
            "Traditional Distillation: Epoch [4/10], Total Loss: 0.4254\n",
            "Traditional Distillation: Epoch 5 Batch 0, Loss_hard: 0.4678, Loss_KD: 0.0993\n",
            "Traditional Distillation: Epoch [5/10], Total Loss: 0.4221\n",
            "Traditional Distillation: Epoch 6 Batch 0, Loss_hard: 0.4367, Loss_KD: 0.0983\n",
            "Traditional Distillation: Epoch [6/10], Total Loss: 0.4207\n",
            "Traditional Distillation: Epoch 7 Batch 0, Loss_hard: 0.4461, Loss_KD: 0.1000\n",
            "Traditional Distillation: Epoch [7/10], Total Loss: 0.4198\n",
            "Traditional Distillation: Epoch 8 Batch 0, Loss_hard: 0.4279, Loss_KD: 0.1071\n",
            "Traditional Distillation: Epoch [8/10], Total Loss: 0.4200\n",
            "Traditional Distillation: Epoch 9 Batch 0, Loss_hard: 0.4431, Loss_KD: 0.0966\n",
            "Traditional Distillation: Epoch [9/10], Total Loss: 0.4205\n",
            "Traditional Distillation: Epoch 10 Batch 0, Loss_hard: 0.4226, Loss_KD: 0.1009\n",
            "Traditional Distillation: Epoch [10/10], Total Loss: 0.4204\n",
            "\n",
            "=== Training Student Model via Step-by-Step Distillation ===\n",
            "Step-by-Step Distillation: Epoch 1 Batch 0, Loss_hard: 0.6940, Loss_rationale: 0.8516, Loss_KD: 0.0881, Loss_consistency: 0.0001, Current_beta: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step-by-Step Distillation: Epoch [1/10], Total Loss: 0.4429\n",
            "Step-by-Step Distillation: Epoch 2 Batch 0, Loss_hard: 0.4436, Loss_rationale: 0.8945, Loss_KD: 0.1044, Loss_consistency: 0.0001, Current_beta: 0.0125\n",
            "Step-by-Step Distillation: Epoch [2/10], Total Loss: 0.4269\n",
            "Step-by-Step Distillation: Epoch 3 Batch 0, Loss_hard: 0.4254, Loss_rationale: 0.8320, Loss_KD: 0.1006, Loss_consistency: 0.0001, Current_beta: 0.0250\n",
            "Step-by-Step Distillation: Epoch [3/10], Total Loss: 0.4323\n",
            "Step-by-Step Distillation: Epoch 4 Batch 0, Loss_hard: 0.3987, Loss_rationale: 0.8828, Loss_KD: 0.1119, Loss_consistency: 0.0001, Current_beta: 0.0375\n",
            "Step-by-Step Distillation: Epoch [4/10], Total Loss: 0.4403\n",
            "Step-by-Step Distillation: Epoch 5 Batch 0, Loss_hard: 0.4223, Loss_rationale: 0.8672, Loss_KD: 0.1050, Loss_consistency: 0.0002, Current_beta: 0.0500\n",
            "Step-by-Step Distillation: Epoch [5/10], Total Loss: 0.4447\n",
            "Step-by-Step Distillation: Epoch 6 Batch 0, Loss_hard: 0.3993, Loss_rationale: 0.8359, Loss_KD: 0.1103, Loss_consistency: 0.0001, Current_beta: 0.0625\n",
            "Step-by-Step Distillation: Epoch [6/10], Total Loss: 0.4513\n",
            "Step-by-Step Distillation: Epoch 7 Batch 0, Loss_hard: 0.4091, Loss_rationale: 0.9492, Loss_KD: 0.1038, Loss_consistency: 0.0001, Current_beta: 0.0750\n",
            "Step-by-Step Distillation: Epoch [7/10], Total Loss: 0.4576\n",
            "Step-by-Step Distillation: Epoch 8 Batch 0, Loss_hard: 0.4056, Loss_rationale: 0.9688, Loss_KD: 0.1040, Loss_consistency: 0.0001, Current_beta: 0.0875\n",
            "Step-by-Step Distillation: Epoch [8/10], Total Loss: 0.4663\n",
            "Step-by-Step Distillation: Epoch 9 Batch 0, Loss_hard: 0.4085, Loss_rationale: 0.8711, Loss_KD: 0.1065, Loss_consistency: 0.0001, Current_beta: 0.1000\n",
            "Step-by-Step Distillation: Epoch [9/10], Total Loss: 0.4735\n",
            "Step-by-Step Distillation: Epoch 10 Batch 0, Loss_hard: 0.4083, Loss_rationale: 0.8242, Loss_KD: 0.1021, Loss_consistency: 0.0001, Current_beta: 0.1000\n",
            "Step-by-Step Distillation: Epoch [10/10], Total Loss: 0.4726\n",
            "\n",
            "Traditional Distillation Student Predictions (first 10 samples):\n",
            "Sample 1: True Label: 1, Prediction: 0.6131\n",
            "Sample 2: True Label: 1, Prediction: 0.7008\n",
            "Sample 3: True Label: 1, Prediction: 0.7614\n",
            "Sample 4: True Label: 1, Prediction: 0.8580\n",
            "Sample 5: True Label: 1, Prediction: 0.8208\n",
            "Sample 6: True Label: 0, Prediction: 0.3608\n",
            "Sample 7: True Label: 0, Prediction: 0.3100\n",
            "Sample 8: True Label: 1, Prediction: 0.6953\n",
            "Sample 9: True Label: 1, Prediction: 0.6418\n",
            "Sample 10: True Label: 0, Prediction: 0.3004\n",
            "\n",
            "Step-by-Step Distillation Student Predictions (first 10 samples):\n",
            "Sample 1: True Label: 1, Prediction: 0.6674\n",
            "Sample 2: True Label: 1, Prediction: 0.7428\n",
            "Sample 3: True Label: 1, Prediction: 0.7798\n",
            "Sample 4: True Label: 1, Prediction: 0.8996\n",
            "Sample 5: True Label: 1, Prediction: 0.8105\n",
            "Sample 6: True Label: 0, Prediction: 0.3548\n",
            "Sample 7: True Label: 0, Prediction: 0.3183\n",
            "Sample 8: True Label: 1, Prediction: 0.7215\n",
            "Sample 9: True Label: 1, Prediction: 0.7418\n",
            "Sample 10: True Label: 0, Prediction: 0.2965\n"
          ]
        }
      ]
    }
  ]
}