import torch
import math

def compute_attention_scores(Q, K):
    # Q: (B, H, L_q, D), K: (B, H, L_k, D)
    # transpose æœ€åä¸¤ä¸ªç»´åº¦ï¼Œå‡†å¤‡åš Q x K^T
    K_T = K.transpose(-2, -1)  # â†’ (B, H, D, L_k)
    return torch.matmul(Q, K_T)  # â†’ (B, H, L_q, L_k)

def dot_product_attention(Q, K, V, mask=None):
    """
    æ ‡å‡† Scaled Dot Product Attention è®¡ç®—(æ”¯æŒå¤šå¤´): 
    1. scores = QK^T / sqrt(d_k)
    2. softmax(scores)
    3. åŠ æƒæ±‚å’Œ: softmax x V
    """
    d_k = Q.size(-1)  # æ¯ä¸ª head çš„ depth
    scores = compute_attention_scores(Q, K) / math.sqrt(d_k)

    if mask is not None:
        scores = scores.masked_fill(mask == 0, float('-inf'))  # å±è”½æ‰æ— æ•ˆä½ç½®

    weights = torch.softmax(scores, dim=-1)  # æ³¨æ„åŠ›æƒé‡ (B, H, L_q, L_k)
    return torch.matmul(weights, V)          # è¾“å‡º shape: (B, H, L_q, D)

def split_heads(x, num_heads):
    """
    æŠŠè¾“å…¥ (B, L, D) æ‹†æˆå¤šå¤´è¡¨ç¤º â†’ (B, num_heads, L, depth)

    æ€è·¯: 
    1. åŸå§‹å‘é‡æ˜¯ D ç»´(ä¾‹å¦‚ D=8)ï¼Œè¦æ‹†æˆ H ä¸ªå¤´(ä¾‹å¦‚ H=2)
    2. æ¯ä¸ªå¤´çš„ç»´åº¦ = D / H = 4
    3. reshape å…ˆæŠŠ D æ‹†æˆ (H, depth)ï¼Œå˜æˆ (B, L, H, depth)
    4. transpose: äº¤æ¢ H å’Œ L â†’ (B, H, L, depth)

    ğŸ“Œ ä¸ºä»€ä¹ˆäº¤æ¢ï¼Ÿ
    - å› ä¸ºæˆ‘ä»¬å¸Œæœ› **æ¯ä¸ªå¤´èƒ½ç‹¬ç«‹å¤„ç†ä¸€æ•´å¥(seq_len)**
    - æ‰€ä»¥è¦æŠŠ â€œæ¯ä¸ª token çš„å¤šä¸ªå¤´å‘é‡â€ å˜æˆ â€œæ¯ä¸ªå¤´çš„ä¸€æ•´ç»„ tokenâ€
    """
    B, L, D = x.shape
    depth = D // num_heads
    x = x.reshape(B, L, num_heads, depth)    # (B, L, H, D') â† æŠŠ d_model æ‹†æˆå¤šä¸ªå¤´
    return x.permute(0, 2, 1, 3)             # (B, H, L, D') â† æ¯ä¸ªå¤´è‡ªå·±çœ‹ä¸€æ•´å¥

def combine_heads(x):
    """
    å°†å¤šä¸ªå¤´çš„è¾“å‡ºé‡æ–°æ‹¼æ¥å›åŸå§‹ç»´åº¦ (B, L, D_model)

    æ€è·¯: 
    1. åŸå§‹å¤šå¤´è¾“å‡ºæ˜¯ (B, H, L, D)
    2. æˆ‘ä»¬æƒ³è¦ (B, L, H * D)
    3. æ‰€ä»¥å…ˆ permuteï¼ŒæŠŠ L æ”¾å›ç¬¬äºŒç»´ï¼Œå† reshape æ‹¼æ¥å¤´
    """
    B, H, L, D = x.shape
    return x.permute(0, 2, 1, 3).reshape(B, L, H * D)

# ========================= æµ‹è¯• =========================

# å‡è®¾: 1 ä¸ª batchï¼Œå¥å­é•¿åº¦ 4ï¼Œæ¯ä¸ª token æ˜¯ 8 ç»´å‘é‡
batch_size, seq_len, d_model, num_heads = 1, 4, 8, 2
depth = d_model // num_heads

x = torch.randn(batch_size, seq_len, d_model)  # æ¨¡æ‹Ÿè¾“å…¥å¥å­ (B, L, D)

# æ‹†åˆ†å¤šå¤´: Q, K, V éƒ½ä» x æ‹·è´
Q = split_heads(x, num_heads)  # â†’ (B, H, L, D')
K = split_heads(x, num_heads)
V = split_heads(x, num_heads)

attn_output = dot_product_attention(Q, K, V)  # â†’ (B, H, L, D')
output = combine_heads(attn_output)          # â†’ (B, L, D_model)

# ========================= æ‰“å°ç»´åº¦ =========================
print("è¾“å…¥ shape:", x.shape)                     # (1, 4, 8)
print("æ‹†å¤šå¤´å Q shape:", Q.shape)              # (1, 2, 4, 4)
print("Attention è¾“å‡º shape:", attn_output.shape) # (1, 2, 4, 4)
print("æ‹¼æ¥åè¾“å‡º shape:", output.shape)          # (1, 4, 8)
